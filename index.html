<!DOCTYPE html>
<html lang="en">
<head>
  <basefont size="10">
  <title>Workshop on extracting structured knowledge from scientific publications</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./css/bootstrap.min.css">
  <script src="./js/bootstrap.min.js"></script>
<!--   <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script> -->
<!--   <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"></script> -->
  <style>
  .fakeimg {
      height: 200px;
      background: #aaa;
  }
   html, body {
        margin: 0;
        border: 0;
        width: 100%;
    }
    body {
        padding: 0 30px;
    }

  </style>
</head>

<body>
<!-- 
<div class="jumbotron text-center" style="margin-bottom:0">
  <h1>My First Bootstrap 4 Page</h1>
  <p>Resize this responsive page to see the effect!</p> 
</div>
-->

<nav class="navbar sticky-top navbar-expand-sm bg-dark navbar-dark">
  <a class="navbar-brand" href="#">Structured knowledge from scientific publications</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="collapsibleNavbar">
    <ul class="navbar-nav">
      <li class="nav-item">
        <!--<a class="nav-link" href="#cfp">Call for Papers</a>-->
        <a class="nav-link" href="AcceptedPapers.html">Accepted Papers</a>
      </li>
      <li class="nav-item">
        <!--<a class="nav-link" href="InvitedSpeakers.html">Invited Speakers</a>-->
        <a class="nav-link" href="#is">Invited Speakers</a>
      </li>    
      <li class="nav-item">
        <a class="nav-link" href="#oc">Organization</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#dates">Important Dates</a>
      </li>    
      <!--<li class="nav-item">
        <a class="nav-link" href="#ref">References</a>
      </li> -->    
    </ul>
  </div>  
</nav>

<p>
<h2>Workshop on extracting structured knowledge from scientific publications (ESSP)</h2>
  
  <h3>June 6th, 2019, Minneapolis, USA</h3>  
 
  <br>
  <h3>Collocated with <a href="https://naacl2019.org/">NAACL 2019</a> </h3>  

  <p>
Scientific knowledge is one of the greatest assets of humankind. 
This knowledge is recorded and disseminated in scientific publications, and the body of scientific literature is growing at an enormous rate.
Automatic methods of processing and cataloguing that information are necessary for assisting scientists to navigate this vast amount of information, and for facilitating automated reasoning, discovery and decision making on that data.
</p>
  <p>
Structured information can be extracted at different levels of granularity.
Previous and ongoing work has focused on bibliographic information (segmentation and linking of referenced literature, Wick et al., 2013), keyword extraction and categorization (e.g., what are tasks, materials and processes central to a publication, (Augenstein et al., 2017)), and cataloguing research findings. Scientific discoveries can often be represented as pairwise relationships, e.g., protein-protein (Mallory et al., 2016), drug-drug (Segura-Bedmar et al., 2013), and chemical-disease (Li et al., 2016) interactions, or as more complicated networks such as action graphs describing scientific procedures (e.g., synthesis recipes in material sciences, (Mysore et al., 2017)). Information extracted with such methods can be enriched with time-stamps, and other meta-information, such as indicators of uncertainty or limitations of the discovered facts (Zhou et al., 2015).
  </p>
Structured representations, such as knowledge graphs, summarize information from a variety of sources in a convenient and machine readable format. Graph representations, that link the information of a large body of publications, can reveal patterns and lead to the discovery of new information that would not be apparent from the analysis of just one publication (Luan et al., 2018). This kind of aggregation can lead to new scientific insights (Kim et al., 2017), and it can also help to detect trends (Prabhakaran et al., 2016), or find experts for a particular scientific area (Neshati et al., 2014).
</p>
  <p>
While various workshops have focused separately on several aspects -- extraction of information from scientific articles, building and using knowledge graphs, the analysis of bibliographical information, graph algorithms for text analysis -- the proposed workshop focuses on processing scientific articles and creating structured repositories such as knowledge graphs for finding new information and making scientific discoveries.
The aim of this workshop is to identify the necessary representations for facilitating automated reasoning over scientific information, and to bring together experts in natural language processing and information extraction with scientists from other domains (e.g. material sciences, biomedical research) who want to leverage the vast amount of information stored in scientific publications.
  </p>
  </p>
  
<br id="is">
<br />  
<h3>Invited speakers</h3>

<ul>
  <li><b> Hoifung Poon</b> <br />Director, Precision Health NLP @ Microsoft <br />
  <b>Machine Reading for Precision Medicine</b> <br ?>
    <p>The advent of big data promises to revolutionize medicine by making it more personalized and effective, but big data also presents a grand challenge of information overload. For example, tumor sequencing has become routine in cancer treatment, yet interpreting the genomic data requires painstakingly curating knowledge from a vast biomedical literature, which grows by thousands of papers every day. Electronic medical records contain valuable information for drug development and clinical trial matching, but curating such real-world data from clinical notes can take hours for a single patient. NLP can play a key role in interpreting big data for precision medicine. In particular, machine reading can help unlock knowledge from text by substantially improving curation efficiency. However, standard supervised methods require labeled examples, which are expensive and time-consuming to produce at scale.</p>
<p>In this talk, I'll present Project Hanover, where we overcome the annotation bottleneck by combining deep learning with probabilistic logic, and by exploiting indirect supervision from readily available resources such as ontologies and databases. This enables us to extract knowledge from millions of publications, reason efficiently with the resulting knowledge graph by learning neural embeddings of biomedical entities and relations, and apply the extracted knowledge and learned embeddings to supporting precision oncology.	</p> <br />
    <p><i>Bio:</i> Hoifung Poon is the Director of Precision Health NLP at Microsoft Research and an affiliated faculty at the University of Washington Medical School. He leads Project Hanover, with the overarching goal of advancing machine reading for precision health, by combining probabilistic logic with deep learning. He has given tutorials on this topic at top conferences such as the Association for Computational Linguistics (ACL) and the Association for the Advancement of Artificial Intelligence (AAAI). His research spans a wide range of problems in machine learning and natural language processing (NLP), and his prior work has been recognized with Best Paper Awards from premier venues such as the North American Chapter of the Association for Computational Linguistics (NAACL), Empirical Methods in Natural Language Processing (EMNLP), and Uncertainty in AI (UAI). He received his PhD in Computer Science and Engineering from University of Washington, specializing in machine learning and NLP.
    </p>
  </li>
  <br /> 
  <li> <b>Dina Demner-Fushman</b> <br />National Library of Medicine<br />  <b>Extracting structured knowledge from biomedical publications</b> <br /> <p>
      The National Library of Medicine (NLM) plays a pivotal role in translating biomedical research into practice. One of the foundational tasks in supporting NLM goals is biomedical language processing. The talk will introduce NLM’s resources for biomedical and clinical NLP -- natural language processing methods to support healthcare by operationalizing clinical information contained in the biomedical literature and clinical narrative.
The talk will then focus on the approaches to extraction of structured knowledge from biomedical publications, highlighting automation and support of manual indexing of biomedical literature with Medical Subject Headings (MeSH), as well as extraction of information from tables and the full text of articles. </p>
    <br /> <p><i>Bio:</i> Dina Demner-Fushman, Investigator, leads research in information retrieval and natural language processing at the National Library of Medicine. Dina earned a doctor of medicine degree from Kazan State Medical Institute, a clinical research Doctorate degree (PhD) in Medical Sciences from Moscow Medical and Stomatological Institute, and MS and PhD degrees in Computer Science from the University of Maryland. She is the author of more than 190 articles and book chapters in the fields of information retrieval, natural language processing, and biomedical and clinical informatics. She is a Fellow of the American College of Medical Informatics (ACMI), an Associate Editor of the Journal of the American Medical Informatics Association, and one of the founding members of the Association for Computational Linguistics Special Interest Group on biomedical natural language processing (SIGBioMed).
</p>
  </li>
   <br />
  <li><b> Michael Cafarella</b> <br /> University of Michigan <br />
  <b>Extraction-Intensive Systems for the Social Sciences</b><br />
  <p>Knowledge extraction technologies have improved dramatically in recent years and have started to make an impact on practical scientific discovery. This talk will survey recent work in applying extraction methods to scientific problems, and in particular two recent efforts in the social sciences.</p>
 <p>The first is RaccoonDB, a declarative nowcasting data management system, which enables users to predict real-world time-series phenomena from extracted social media signals. RaccoonDB’s novel query optimization methods allow it to generate useful social science predictions 123 times faster than competing systems, using just 10% of the computational resources. When applied to unemployment phenomena, the system yields predictions with accuracy that is comparable to predictions from real-world economists.</p>
<p>The second system is an information extraction system designed to analyze online text and help law enforcement officers identify potential human trafficking victims. This system has been successfully applied to real-world cases. In addition, the resulting extracted dataset enables several novel social science findings about behavior in an illicit and often opaque market.</p><br />
    <p><i>Bio:</i> Michael Cafarella is an Associate Professor of Computer Science and Engineering at the University of Michigan. His research interests include databases, information extraction, data integration, and data mining. He has published extensively in venues such as SIGMOD, VLDB, and elsewhere. Mike received his PhD from the University of Washington in 2009 with advisors Oren Etzioni and Dan Suciu. His academic awards include the NSF CAREER award, the Sloan Research Fellowship, and the VLDB Test of Time Award. In addition to his academic work, Mike cofounded (with Doug Cutting) the Hadoop open-source project. In 2015 he cofounded (with Chris Re and Feng Niu) Lattice Data, Inc., which is now part of Apple.</p>
  </li>
    <br /> 
    
  <li> <b>Chris Welty</b> <br /> Google AI <br /> <b> Just when I thought I was out, they pull me back in: 
    The role of knowledge representation in automatic knowledge base construction </b> <br /> 
    <p> Automatic construction of knowledge-bases is an exciting synthesis of research in machine learning, knowledge representation, natural language processing, and recently, crowdsourcing.  Due to the ever increasing over-speciation of researchers in AI, most work in the area strongly emphasizes one of these four areas, and as a result, de-emphasizes the others.  Bringing these diverse elements together requires learning a few things from each other, which, when treated as equals, can force us to re-evaluate tacit assumptions that one field makes and another throws away.  My own journey from formal knowledge representation and ontologies thru building Watson and more recent AI systems at Google Research, led me to re-evaluate the long-held tacit assumptions of the knowledge representation field and come to a new understanding of its role in AI</p>
  </li>

</ul>

<br id="cfp">
<br>

<div class='card card-body bg-light'>
  <h2> Call for Papers</h2>  
  
  <h3><a href="AcceptedPapers.html">Accepted Papers</a></h3>

  
<b>We invite submissions on (but not limited to) the following topics:</b><br>
<ul>
<li>Information extraction from scientific publications</li>

<ul>
<li>identification of concepts in scientific articles (in various domains)</li>
<li>extraction of relations from scientific articles (in various domains) — including n-ary relations with n>2, “negative relations”</li>
<li>large scale information extraction, clustering and detection of trends in scientific fields</li>
<li>targeted information extraction for completing knowledge graphs</li>
<li>updating knowledge graphs (adding new information, removing erroneous facts, or possibly having explicit links for incorrect statements)</li>
</ul>

<li>finding patterns and mining new information in knowledge graphs</li>

<ul>
<li>automatic generation and ranking of scientific hypotheses</li>
  <li>aggregation and extraction of human-understandable scientific rules and generalities</li>
  <li>extraction of script-knowledge and scientific procedures</li>
  <li>detection of (inferred or explicitly stated) causality</li>
  <li>automated reasoning over repositories of extracted information</li>
</ul>

<li>Using extracted structured knowledge</li>
  <ul>
    <li>visualization of knowledge in particular domains</li>
    <li>tools for interacting with users</li>
    <li>querying knowledge graphs/knowledge repositories</li>
    <li>evaluation of extracted knowledge</li>
  </ul>

</ul>
  
  Further <a href="Call_for_papers.html">submission information and submission link</a>
  
</div>

<br id="dates">
<br>


<h2>Important Dates</h2>
  <b>Workshop papers due:</b> 	Wednesday 	March 6, 2019 <br />
  <b>Notification of acceptance:</b> 	Friday 	March 29, 2019 <br />
  <b>Camera-ready papers due (firm deadline):</b> 	Friday 	April 5, 2019 <br />
  <b>Workshop dates:</b> 	Thursday	June 6th, 2019 <br />

<br id="oc">
<br>

  
 
<h2>Organization</h2>

  <h4>Organizing Committee</h4>
  <br>

<ul>
<li>Vivi Nastase, Heidelberg University</li>
<li>Benjamin Roth, Ludwig Maximilian University, Munich</li>
<li>Laura Dietz, University of Newhampshire</li>
<li>Andrew McCallum, University of Massachusetts, Amherst</li>
</ul>
  
<br>
  <h4><a href="PC.html">Program Committee</a></h4>

<br id="ref">
<br>

  <h2>References</h2>

  <ul>
  <li>Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. 2017.  <i>SemEval 2017 task
    10:  ScienceIE - extracting keyphrases and relations from scientific publications.</i> (SemEval-2017) </li>    
  <li> Edward Kim, Kevin Huang, Adam Saunders, Andrew McCallum, Gerbrand Ceder, and Elsa Olivetti. 2017. <i>Materials synthe-
    sis insights from scientific literature via text extraction and machine learning.</i> Chemistry of Materials 29(21) </li>
  <li>Jiao Li,  Yueping Sun,  Robin J Johnson,  Daniela Sciaky,  Chih-Hsuan Wei,  Robert Leaman,  Allan Peter Davis,  Carolyn J
Mattingly,  Thomas  C  Wiegers,  and  Zhiyong  Lu.  2016.    <i>Biocreative  VCDR  task  corpus:   a  resource  for  chemical  dis-
    ease  relation  extraction.</i> Database : the journal of biological databases and curation. </li>
  <li>Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2017.  <i>Scientific Information Extraction with Semi-supervised Neural Tagging</i> (EMNLP) </li>
  <li>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018.  <i>Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction</i> (EMNLP) </li>
    <li>Emily K. Mallory, Ce Zhang, Christopher Re, and Russ B. Altman. 2016. <i> Large-scale extraction of gene interactions from
    full-text literature using DeepDive.</i> Bioinformatics 32(1):106–113. </li>
    <li>Sheshera Mysore, Edward Kim, Emma Strubell, Ao Liu, Haw-Shiuan Chang, Srikrishna Kompella, Kevin Huang, Andrew
McCallum, and Elsa Olivetti. 2017. <i> Automatically extracting action graphs from materials science synthesis procedures</i>.</li>
    <li>Mahmood Neshati, Djoerd Hiemstra, Ehsaneddin Asgari, and Hamid Beigy. 2014.  <i> Integration of scientific and social networks.</i> World Wide Web 17(5) </li>
    <li>Vinodkumar Prabhakaran,  William L. Hamilton,  Dan McFarland,  and Dan Jurafsky. 2016.   <i>Predicting the rise and fall of
      scientific topics from trends in their rhetorical framing. </i> (ACL 2016) </li>
    <li>Isabel Segura-Bedmar, Paloma Martınez, and Marıa Herrero Zazo. 2013. <i>SemEval-2013 task 9 : Extraction of drug-drug inter-
      actions from biomedical texts </i>(DDIExtraction 2013). (*SEM) </li>
    <li>Michael L Wick, Ari Kobren, and Andrew McCallum. 2013.  <i>Large-scale author coreference via hierarchical entity represen-
      tations.</i> (PEER) </li>
  <li>Huiwei Zhou, Huijie Deng, Degen Huang, and Minling Zhu. 2015.  <i>Hedge scope detection in biomedical texts: An effective
    dependency-based method.</i> PLoS One 10(7) </li>
  </ul>
  
  <br>
  
</body>
</html>
