<!DOCTYPE html>
<html lang="en">
<head>
  <basefont size="10">
  <title>Workshop on Weakly Supervised Learning (WeaSuL)</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./css/bootstrap.min.css">
  <script src="./js/bootstrap.min.js"></script>
<!--   <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script> -->
<!--   <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"></script> -->
  <style>
  .fakeimg {
      height: 200px;
      background: #aaa;
  }
   html, body {
        margin: 0;
        border: 0;
        width: 100%;
    }
    body {
        padding: 0 30px;
    }

  </style>
</head>

<body>
<!-- 
<div class="jumbotron text-center" style="margin-bottom:0">
  <h1>My First Bootstrap 4 Page</h1>
  <p>Resize this responsive page to see the effect!</p> 
</div>
-->

<nav class="navbar sticky-top navbar-expand-sm bg-dark navbar-dark">
  <a class="navbar-brand" href="#">Weakly Supervised Learning</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="collapsibleNavbar">
    <ul class="navbar-nav">
      <li class="nav-item">
        <!--<a class="nav-link" href="InvitedSpeakers.html">Invited Speakers</a>-->
        <a class="nav-link" href="#is">Invited Speakers</a>
      </li>    
       <li class="nav-item">
        <a class="nav-link" href="#cfp">Call for Papers</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#dates">Important Dates</a>
      </li>   
      <li class="nav-item">
        <a class="nav-link" href="#oc">Organization</a>
      </li>
 
      <!--<li class="nav-item">
        <a class="nav-link" href="#ref">References</a>
      </li> -->    
    </ul>
  </div>  
</nav>

<p>
<h2>Workshop on Weakly Supervised Learning (WeaSuL)</h2>
  
  <h3>2021, virtual</h3> 
  <br>
  <h3>Collocated with <a href="https://iclr.cc/">ICLR 2021</a> </h3>  

  <p>
Deep learning relies on massive training sets of labeled examples to learn from - often tens of thousands to millions to reach peak predictive performance. However, large amounts of training data are only available for very few standardized learning problems. Even small variations of the problem specification or changes in the data distribution would necessitate re-annotation of large amounts of data.
  </p>
  <p>
However, domain knowledge can often be expressed by sets of prototypical descriptions: For example, vision experts can exploit meta information for image labeling, linguists can describe discourse phenomena by prototypical realization patterns, social scientists can specify events of interest by characteristic key phrases, and bio-medical researchers have databases of known interactions between drugs or proteins that can be used for heuristic labeling. These knowledge-based descriptions can be either used as rule-based predictors or as labeling functions for providing partial data annotations. The growing field of weak supervision provides methods for refining and generalizing such heuristic-based annotations in interaction with deep neural networks and large amounts of unannotated data.
  </p>
  <p>
In this workshop, we want to advance theory, methods and tools for allowing experts to express prior coded knowledge for automatic data annotations that can be used to train arbitrary deep neural networks for prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing methods that help modern machine-learning methods to generalize from knowledge provided by experts, in interaction with observable (unlabeled) data.
  </p>
  <p>
   In particular, we are interested in the following questions:
<ul>
<li> What tasks, not traditionally solved with weak supervision, can profit from knowledge-based labeling?</li>
<li> What are the typical characteristics of a prediction task that make it amenable to weak supervision? </li>
<li> How can weak supervision best be combined with neural networks and representation learning? </li>
<li> What is the relationship of weak supervision to other machine learning paradigms, specifically,
to semi-supervised learning,
to active-learning,
to label denoising and confidence estimation? </li>
<li> Can approaches to weak supervision from different fields, e.g, relation extraction (natural language processing) and image classification (vision) be unified? </li>
<li> What are good benchmarks for evaluating and comparing weak supervision approaches? </li>
</ul>
   </p>
  <p>
    Learning with weak supervision is both studied from a theoretical perspective as well as applied to a variety of tasks from areas like natural language processing and computer vision. This workshop aims at bringing together researchers from this wide range of fields to facilitate discussions across research areas that share the common ground of using weak supervision. A target of this workshop is also to inspire applications of weak supervision to new scenarios and to enable researchers to work on tasks that so far have been considered too low-resource.
   </p>
  <p>
As weak supervision addresses one of the major issues of current machine learning techniques, the lack of labeled data, it has also started to obtain commercial interest. This workshop is an opportunity to bridge innovations from academia and the requirements of industry settings.
  </p>
  </p>
  
<br id="is">
<br />  
<h3>Invited speakers</h3>

<ul>
  <li><b>Dan Roth</b> <br /> University of Pennsylvania <br />
    <p><i>Bio:</i> Dan Roth is a Distinguished Professor at the Department of Computer and Information Science, University of Pennsylvania. His research focuses on the computational foundations of intelligent behavior. He and his lab develop theories and systems pertaining to intelligent behavior using a unified methodology -- at the heart of which is the idea that learning has a central role in intelligence.  Recent work has emphasized the notion of incidental supervision as a way to get around the inherent difficulty in supervising complex problems. Over the last decade his lab has developed a declarative Learning Based Programming language, LBJava, for the rapid development of software systems with learned components; and is currently working on Saul, a next generation Declarative Learning Based Program.</p>
  </li>

  
  <br />
  <li><b> Paroma Varma</b> <br /> Snorkel AI <br />
    <p><i>Bio:</i> Paroma Varma is a co-founder of Snorkel AI, an AI start-up based on the influential Snorkel project. Snorkel AI provides a data-first platform for building, managing, and monitoring end-to-end AI applications.
Paroma received her Ph.D. from Stanford University and she was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship. Her research interests revolve around weak supervision, or using high-level knowledge in the form of noisy labeling sources to efficiently label massive datasets required to train machine learning models. In this context, she is also interested in using developer exhaust, byproducts of the data analytics pipeline, to simplify complex statistical and search-based problems.
</p>
  </li>

  
  <br />
  <li><b> Heng Ji</b> <br /> University of Illinois at Urbana-Champaign <br />
    <p><i>Bio:</i> Heng Ji is a professor of Computer Science at the University of Illinois at Urbana-Champaign. Her research interests focus on Natural Language Processing and its connections with Data Mining, Social Science and Vision. Recent work focuses on weak supervision methods for schema-guided event understanding.
Heng Ji was selected as "Young Scientist" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017. The awards she received include "AI's 10 to Watch" Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, "Best of ICDM2013" paper award, "Best of SDM2013" paper award, ACL2018 Best Demo paper nomination, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She has coordinated the NIST TAC Knowledge Base Population task since 2010 and led several multi-institute research efforts including DARPA DEFT Tinker Bell team of seven universities and DARPA KAIROS RESIN team of six universities.
</p>
  </li>

  
  <br />
  <li><b> Lu Jiang</b> <br /> Google Research <br />
    <p><i>Bio:</i> Lu Jiang is a senior research scientist at Google Research. He obtained his Ph.D. at Carnegie Mellon University. His research goal is to solve realistic problems on big multimodal data. His recent work on weak and unreliable supervision includes approaches like MentorNet and a recent dataset for noisy web images. His work on robust machine translation was nominated for best paper at ACLâ€™19.</p>
  </li>
  
  <br />
  <li><b> Marine Carpuat</b> <br /> University of Maryland <br />
    <p><i>Bio:</i> Marine Carpuat is an Assistant Professor of Computer Science at the University of Maryland. She received a PhD in Computer Science from Hong Kong University. Her research interests are in Natural Language Processing and Machine Translation. Recent work of hers includes the use of weak supervision for cross-lingual classification. Marine is the recipient of an NSF CAREER award, research awards from Google and Amazon and best paper awards at the *SEM and TALN conferences.</p>
  </li>

</ul>

<br id="prog">
<br>
  
<br id="cfp">
<br>

<div class='card card-body bg-light'>
  <h2> Call for Papers</h2>  
  
  
<b>We invite submissions on (but not limited to) the following topics:</b><br>
<ul>  
<li> Weak supervision in combination with neural networks and representation learning</li>
<li> Theoretic insights into weak supervision</li>
<li> Relationship between weak supervision and other machine learning paradigms incl. semi-supervised learning, active learning and label denoising</li>
<li> Distant supervision and weak supervision for specific tasks</li>
<li> Interdisciplinary applications of weak supervision</li>
<li> Unification of weak supervision approaches from different fields, e.g, relation extraction (natural language processing) and image classification (vision)</li>
<li> Analysis of failure cases of weak supervision</li>
<li> Benchmarks for evaluating and comparing weak supervision approaches</li>
<li> Applications of weak supervision in industry settings</li>
  </ul>  
</div>

<br id="dates">
<br>


<h2>Important Dates</h2>
  <b>Feb 26, 2021:</b> Paper Submission Deadline<br />
  <b>Mar 26, 2021:</b> Author Notification<br />
  <b>May 8, 2021:</b> Workshop Date<br />

<br id="oc">
<br>


<h2>Submission Instructions</h2>
  <p>
 We solicit two categories of papers: long and short papers. Authors can decide the archival status of their publications. Submissions will go through a double-blind review process, where each submission is reviewed by at least two program committee members. 
  </p>
  <p>
Accepted papers will be presented by the authors either as talk or poster. All submissions must follow the ICLR 2021 formatting requirements (<a href="https://iclr.cc/Conferences/2021/CallForPapers">https://iclr.cc/Conferences/2021/CallForPapers</a>). 
<ul>  
<li> Short paper submission: up to 4 pages of content (+1 on acceptance), plus bibliography </li>
<li>Long paper submission: up to 8 pages of content (+1 on acceptance), plus bibliography
 </li>
  </ul> 
  </p>

  
 
<h2>Organization</h2>

  <h4>Organizing Committee</h4>
  <br>

<ul>
<li>Benjamin Roth, University of Vienna</li>
<li>Barbara Plank, IT University of Copenhagen</li>
<li>Alex Ratner, University of Washington</li>
<li>Katharina Kann, University of Colorado Boulder</li>
<li>Dietrich Klakow, Saarland University</li>
<li>Michael Hedderich, Saarland Informatics Campus </li>
</ul>
  
<br>  
</body>
</html>
